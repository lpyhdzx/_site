---
layout: post
title:  "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models"
date:   2021-08-04 18:04:44 +00:00
image: images/MPOE.png
categories: research
author: Peiyu Liu
authors: "Ze-Feng Gao*, <strong>Peiyu Liu*</strong>, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen"
venue: "COLING 2022, Oral Presentation, 2022"
paper: https://zefeng-gao.com/pdfs/coling2022.pdf
code: https://github.com/RUCAIBox/MPOE
---
This study proposes a novel approach to reduce the parameter count of the MoE model, by using a shared central tensor among experts, resulting in a 27.2-fold reduction compared to the Switch Transformer.
